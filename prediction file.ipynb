{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3/1imiGmAlJn4pDdFlqEe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLsScX5boFlu","executionInfo":{"status":"ok","timestamp":1685175959139,"user_tz":-180,"elapsed":26002,"user":{"displayName":"asmaa m.mansour","userId":"08012431486127069488"}},"outputId":"4a18a123-1b31-436e-aa52-3557df542c3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/iti/NLP/project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4k_tkmose3S","executionInfo":{"status":"ok","timestamp":1685175959142,"user_tz":-180,"elapsed":15,"user":{"displayName":"asmaa m.mansour","userId":"08012431486127069488"}},"outputId":"4f2af129-8a94-4536-9631-9e29977da85b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/iti/NLP/project\n"]}]},{"cell_type":"code","source":["# import tensorflow as tf\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import word_tokenize\n","import re \n","\n","# from sklearn.svm import LinearSVC\n","from sklearn.pipeline import make_pipeline\n","import preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HynusPXio0TW","executionInfo":{"status":"ok","timestamp":1685175962082,"user_tz":-180,"elapsed":2950,"user":{"displayName":"asmaa m.mansour","userId":"08012431486127069488"}},"outputId":"c5db7c4c-af00-4339-e03f-a600da121c29"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]}]},{"cell_type":"code","source":["import joblib\n","pipe_tfidf = joblib.load('/content/drive/MyDrive/iti/NLP/project/models/tfidf_pipeline.pkl')"],"metadata":{"id":"KouUsepaoRVw","executionInfo":{"status":"ok","timestamp":1685175966152,"user_tz":-180,"elapsed":4084,"user":{"displayName":"asmaa m.mansour","userId":"08012431486127069488"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["mapping = {\"EG\":\"Egypt\", \"LB\": \"Lebanon\", \"LY\": \"Libya\", \"MA\": \"Morocco\", \"SD\":\"Sindhi\"}\n","# text = [\"مش عارفة حاجة\", \"انت فين\", \"ليش\", \"والله اننا نحبك بزاف\",\"شلون\", \"😎ايه ياسسسسطا!   عامل ايه\", \"كان نفسي اداوم اليوم بس عن جد ما خلصت\",\"ولك تسلم على هادي الكورس\"]\n","# text = [processing(x) for x in text]\n","text = [input(\"enter the sentence to predict: \")]\n","text = [preprocessing.processing(x) for x in text]\n","print(text)\n","y_pred = pipe_tfidf.predict(text)\n","for i in range(len(text)):\n","  print(f'{text[i]} --> {mapping[y_pred[i]]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-64xM9gfoRQX","executionInfo":{"status":"ok","timestamp":1685176049456,"user_tz":-180,"elapsed":2651,"user":{"displayName":"asmaa m.mansour","userId":"08012431486127069488"}},"outputId":"158e5fa5-f357-4382-c3e0-13ebe89afff4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["enter the sentence to predict: \"خوي زيكو بابور ليهامعاني تاتية ترا\"\n","['خوي زيكو بابور ليهامعاني تاتية ترا']\n","خوي زيكو بابور ليهامعاني تاتية ترا --> Libya\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SGLbiL82pNqw","executionInfo":{"status":"ok","timestamp":1685175991606,"user_tz":-180,"elapsed":11,"user":{"displayName":"asmaa m.mansour","userId":"08012431486127069488"}}},"execution_count":5,"outputs":[]}]}